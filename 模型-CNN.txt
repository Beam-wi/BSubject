2. 激活函数 relu 作用
    a. 从便于n维w值同时收敛的角度讲，relu的值域[0, x], 单x小于0时y=0，当x>0时y=x，所以添加relu后的神经元输出为 [0, x]间，而本身x做过归一化就到了[0, 1]之间
       这样就能保证输出既下层输入一直处于一个小的值。
    b. 从函数角度讲，加入非线性因子，可以让模型解决线性不可分的问题。比如平面上的点由直线不能分的情况下，可以拟合一条曲线进行区分。
    c. 用relu等激活函数，缓解梯度消失
    d. 吴恩达说加入非线性因子我们就能训练出一个有趣的函数，没有非线性因子函数就永远是在原有的w, b基础上线性重组还是一个线性函数。










输入数据归一化的作用：便于n维w值同时收敛，假如有两个输入值x 一个很大一个很小，从梯度角度看大的那个对w求导倒数会远大于小的那个，
                     而这时学习率是一致的，通过w更新公式 w -= afa*（w导数），这里对w求导导数不就是x吗所以导数大小取决于x大小，
                     这样小的那个可以慢慢收敛到最优 但大的那个就容易震荡。
