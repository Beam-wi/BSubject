1. 卷积核的计算
    输入尺寸（input）： i
    卷积核大小（kernel size）： k
    步幅（stride）： s
    边界扩充（padding）： p
    输出尺寸（output）： o

    o = (i+2p-k)/s + 1

    以下三组尺寸不变
    s = 1时
    k = 3, p = 1
    k = 5, p = 2
    k = 7, p = 3


2. 激活函数 relu 作用
    a. 限定下层输入 从便于n维w值同时收敛的角度讲，relu的值域[0, x], 单x小于0时y=0，当x>0时y=x，所以添加relu后的神经元输出为 [0, x]间，而本身x做过归一化就到了[0, 1]之间
       这样就能保证输出既下层输入一直处于一个小的值。
    b. 空间复杂度 从函数角度讲，加入非线性因子，可以让模型解决线性不可分的问题。比如平面上的点由直线不能分的情况下，可以拟合一条曲线进行区分。
    c. 梯度 用relu等激活函数，缓解梯度消失（因为链式求导会将这部分倒数计算到总的倒数中，而我们的激活函数一般都会选择导数存在的函数，relu左边为零所以有存在一些神经元无法学习的可能，一般与权重的初始化不合理，和数据的输入特性有关可能性比较小）
    d. 吴恩达说加入非线性因子我们就能训练出一个有趣的函数，没有非线性因子函数就永远是在原有的w, b基础上线性重组还是一个线性函数。


3. 池化层的作用
    o = (i-k)/s +1

    a. 下采样（特征图尺寸减小）
    b. 降维（通道减少）；也可以用 c/2 个卷积核做降为，升维也可以
    c. 实现非线性（均值*w + b 类似relu）
    d. 可扩大感知野
    e. 可以实现不变性，其中不变性包括，平移不变性、旋转不变性和尺度不变性







输入数据归一化的作用：便于n维w值同时收敛，假如有两个输入值x 一个很大一个很小，从梯度角度看大的那个对w求导倒数会远大于小的那个，
                     而这时学习率是一致的，通过w更新公式 w -= afa*（w导数），这里对w求导导数不就是x吗所以导数大小取决于x大小，
                     这样小的那个可以慢慢收敛到最优 但大的那个就容易震荡。
