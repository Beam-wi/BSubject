一、 为什么要normalization
 在神经网络学习过程中其实就是为了学习输入数据的分布，而一旦训练数据与测试数据的分布不一样，那么在测试集上的效果就会很差，即网络的泛化能力大大降低。另一方面，如果每个batch_size的训练数据的分布各不相同，那么网络就要在每次迭代都去学习适应不同的分布，这样会大大降低网络的训练速度，所以一般需要对神经网络的输入数据进行归一化处理。归一化的目的就是使得输入数据被限定在一定的范围内（比如[0,1]或者[-1,1]），从而消除个别特殊样本导致的不良影响，在训练开始前将输入数据的变化分布称之为：“covariate shift”。

        虽然已经对输入数据进行归一化处理，但这还不够，因为神经网络每一层训练参数的更新会使得数据的分布发生变化，而且，神经网络前面浅层的微小变化，到后面深层就会逐步把这种改变累积放大，在训练过程中将网络中间层数据分布的改变称之为："Internal Covariate Shift"。 Normalization的提出，就是要解决在训练过程中，网络中间层数据分布发生改变的情况，在每次传入网络的数据每一层的网络都进行一次normalization，将中间层分布发生改变的数据拉回正态分布，使得数据分布一致且避免后面梯度消失的问题。



